{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_available:  True\n",
      "device_count:  1\n",
      "current_device:  0\n",
      "current_device:  <torch.cuda.device object at 0x000001816BACC7C0>\n",
      "get_device_name:  NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "use this to run a simple grid search\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import presets\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models.detection\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "import utils\n",
    "from coco_utils import get_coco, get_coco_kp\n",
    "from engine import evaluate, train_one_epoch\n",
    "from group_by_aspect_ratio import create_aspect_ratio_groups, GroupedBatchSampler\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from transforms import SimpleCopyPaste\n",
    "\n",
    "print('is_available: ', torch.cuda.is_available())\n",
    "print('device_count: ', torch.cuda.device_count())\n",
    "print('current_device: ', torch.cuda.current_device())\n",
    "print('current_device: ', torch.cuda.device(0))\n",
    "print('get_device_name: ', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name, image_set, transform, data_path):\n",
    "    paths = {\"coco\": (data_path, get_coco, 91), \"coco_kp\": (data_path, get_coco_kp, 2)}\n",
    "    p, ds_fn, num_classes = paths[name]\n",
    "\n",
    "    ds = ds_fn(p, image_set=image_set, transforms=transform)\n",
    "    return ds, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir_path = r'C:\\Users\\endle\\Desktop\\pytorch-retinanet\\outputdir'\n",
    "data_dir_path = r\"C:\\Users\\endle\\Desktop\\pytorch-retinanet\\data\"\n",
    "dataset_type = 'coco'\n",
    "model = \"retinanet_resnet50_fpn\"\n",
    "device_type = \"cuda\"\n",
    "batch_size = 8\n",
    "epochs = 3\n",
    "workers = 1\n",
    "optimizer = \"sgd\"\n",
    "norm_weight_decay = 0.9\n",
    "momentum = 0.9\n",
    "lr = 0.0005 #0.001\n",
    "weight_decay = 1e-4\n",
    "lr_step_size = 8\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Data loading code\n",
    "print(\"Loading data\")\n",
    "\n",
    "ds_train, num_t = get_dataset()\n",
    "\n",
    "dataset, num_classes = get_dataset(dataset_type, \"train\", get_transform(True, args), data_dir_path)\n",
    "dataset_test, _ = get_dataset(dataset_type, \"val\", get_transform(False, args), data_dir_path)\n",
    "\n",
    "print(\"Creating data loaders\")\n",
    "if args.distributed:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "    test_sampler = torch.utils.data.distributed.DistributedSampler(dataset_test, shuffle=False)\n",
    "else:\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "if args.aspect_ratio_group_factor >= 0:\n",
    "    group_ids = create_aspect_ratio_groups(dataset, k=args.aspect_ratio_group_factor)\n",
    "    train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, args.batch_size)\n",
    "else:\n",
    "    train_batch_sampler = torch.utils.data.BatchSampler(train_sampler, args.batch_size, drop_last=True)\n",
    "\n",
    "train_collate_fn = utils.collate_fn\n",
    "if args.use_copypaste:\n",
    "    if args.data_augmentation != \"lsj\":\n",
    "        raise RuntimeError(\"SimpleCopyPaste algorithm currently only supports the 'lsj' data augmentation policies\")\n",
    "\n",
    "    train_collate_fn = copypaste_collate_fn\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_sampler=train_batch_sampler, num_workers=args.workers, collate_fn=train_collate_fn\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, sampler=test_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "print(\"Creating model\")\n",
    "kwargs = {\"trainable_backbone_layers\": args.trainable_backbone_layers}\n",
    "if args.data_augmentation in [\"multiscale\", \"lsj\"]:\n",
    "    kwargs[\"_skip_resize\"] = True\n",
    "if \"rcnn\" in args.model:\n",
    "    if args.rpn_score_thresh is not None:\n",
    "        kwargs[\"rpn_score_thresh\"] = args.rpn_score_thresh\n",
    "# model = torchvision.models.get_model(\n",
    "#     args.model, weights=args.weights, weights_backbone=args.weights_backbone, num_classes=num_classes, **kwargs\n",
    "# )\n",
    "\n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True, num_classes=num_classes, **kwargs)\n",
    "\n",
    "model.to(device)\n",
    "if args.distributed and args.sync_bn:\n",
    "    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "\n",
    "model_without_ddp = model\n",
    "if args.distributed:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "    model_without_ddp = model.module\n",
    "\n",
    "if args.norm_weight_decay is None:\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "else:\n",
    "    param_groups = torchvision.ops._utils.split_normalization_params(model)\n",
    "    wd_groups = [args.norm_weight_decay, args.weight_decay]\n",
    "    parameters = [{\"params\": p, \"weight_decay\": w} for p, w in zip(param_groups, wd_groups) if p]\n",
    "\n",
    "opt_name = args.opt.lower()\n",
    "if opt_name.startswith(\"sgd\"):\n",
    "    optimizer = torch.optim.SGD(\n",
    "        parameters,\n",
    "        lr=args.lr,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay,\n",
    "        nesterov=\"nesterov\" in opt_name,\n",
    "    )\n",
    "elif opt_name == \"adamw\":\n",
    "    optimizer = torch.optim.AdamW(parameters, lr=args.lr, weight_decay=args.weight_decay)\n",
    "else:\n",
    "    raise RuntimeError(f\"Invalid optimizer {args.opt}. Only SGD and AdamW are supported.\")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
    "\n",
    "args.lr_scheduler = args.lr_scheduler.lower()\n",
    "if args.lr_scheduler == \"multisteplr\":\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.lr_steps, gamma=args.lr_gamma)\n",
    "elif args.lr_scheduler == \"cosineannealinglr\":\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"Invalid lr scheduler '{args.lr_scheduler}'. Only MultiStepLR and CosineAnnealingLR are supported.\"\n",
    "    )\n",
    "\n",
    "print('what is argsresume: ', args.resume)\n",
    "\n",
    "if args.test_only:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "    return\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    if args.distributed:\n",
    "        train_sampler.set_epoch(epoch)\n",
    "\n",
    "    #print('look at data_loader: ', data_loader) # <torch.utils.data.dataloader.DataLoader object at 0x00000188DE551940>\n",
    "    #print('len of dataloader: ', len(data_loader))\n",
    "\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, args.print_freq, scaler)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    if args.output_dir:\n",
    "        checkpoint = {\n",
    "            \"model\": model_without_ddp.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "            \"args\": args,\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        if args.amp:\n",
    "            checkpoint[\"scaler\"] = scaler.state_dict()\n",
    "        utils.save_on_master(checkpoint, os.path.join(args.output_dir, f\"model_{epoch}.pth\"))\n",
    "        utils.save_on_master(checkpoint, os.path.join(args.output_dir, \"checkpoint.pth\"))\n",
    "\n",
    "    # evaluate after every epoch\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print(f\"Training time {total_time_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
